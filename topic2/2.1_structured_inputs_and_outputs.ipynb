{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsadchikov/LLM-Engineering-Essentials/blob/main/topic2/2.1_structured_inputs_and_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "Author: Alex Umnov\n",
        "\n",
        "Links:\n",
        "- [LinkedIn](www.linkedin.com/in/alex-umnov)\n",
        "- Discord Profile: *alexumnov* , best to tag at #nebius-academy\n",
        "\n",
        "The course is in development now, with more materials coming soon. [Subscribe to stay updated](https://academy.nebius.com/llm-engineering-essentials/update/)\n",
        "# 2.1. Structured Inputs and Outputs\n",
        "\n",
        "In Topic 1, we learnt how to prompt an LLM in such a way that it understands what you want from it and gives a relevant answer. In this notebook we'll continue this discussion by understanding\n",
        "\n",
        "* How to make prompts reusable by using **prompt templates**\n",
        "* How to ensure that an LLM creates its outputs in a convenient, easily parsable format\n",
        "\n",
        "Let's start by running some code which will help us in the whole notebook:"
      ],
      "metadata": {
        "id": "XXuQF3YXe0C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "vmxtcQ2de0DB",
        "outputId": "2a540261-755c-45ab-c1da-9efce2f8851e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/720.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/720.4 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
        "\n",
        "# os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "a1AxEa78e0DB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates"
      ],
      "metadata": {
        "id": "L3wUdp21e0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an LLM-powered system, there's always a layer of prompting logic hidden from the user. For example, ChatGPT, Claude, Gemini and others have quite elaborate **system prompts** that set up rules and guardrails of LLM's communication with the user.\n",
        "\n",
        "However, in some cases a system prompting isn't a flexible enough mechanism. Imagine, for example,\n",
        "\n",
        "* a customer support bot that needs to be aware of the user's geography to give relevant answers about locally available products\n",
        "* a railway service support bot that needs to be aware of today's railway strikes and other calamities\n",
        "\n",
        "You'll likely need to insert this information in the middle of the prompt; and for such things, **prompt templates** are a great tool."
      ],
      "metadata": {
        "id": "xeHMXwGEe0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, a **prompt template** is a template string like\n",
        "\n",
        "```python\n",
        "\"some fixed information {template placeholder 1}\n",
        "some more fixed information {template placeholder 2}\"\n",
        "```\n",
        "\n",
        "where the template placeholders are to be filled in just before an actual LLM call.\n",
        "\n",
        "Let's check several neat ways of wrapping this logic.\n",
        "\n",
        "First of all, you can write your own wrapper. In the example below, `m['content'].format(**kwargs)` allows to put as much formatting as you wish into the user's message."
      ],
      "metadata": {
        "id": "6i-MRNQMZvWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "class MessagesPromptTemplate():\n",
        "    messages: List[Dict]\n",
        "\n",
        "    def __init__(self, messages: List[Dict]):\n",
        "        self.messages = messages\n",
        "\n",
        "    def format(self, **kwargs):\n",
        "        return [\n",
        "            {\n",
        "                \"role\":  m['role'],\n",
        "                \"content\": m['content'].format(**kwargs)\n",
        "            }\n",
        "            for m in self.messages\n",
        "        ]"
      ],
      "metadata": {
        "id": "ZCv8sZ5Be0DC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = MessagesPromptTemplate(\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You only answer in rhymes\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about {city}\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "wrNIeKu3e0DD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.format(city=\"Vilnius\")"
      ],
      "metadata": {
        "id": "kjL-E-bGe0DD",
        "outputId": "66f4055b-2ebc-4dd7-fde0-0ce30986d232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You only answer in rhymes'},\n",
              " {'role': 'user', 'content': 'Tell me about Vilnius'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try calling an llm with different variables"
      ],
      "metadata": {
        "id": "jspgVhL0e0DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Vilnius\"),\n",
        "    model=llama_model\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "DuR5IqTGe0DD",
        "outputId": "3bb4ca8b-764d-443d-fa1f-b5becadf3ae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Vilnius, the capital so fine,\n",
            "Lithuania's beauty does truly shine.\n",
            "A city with history, a story to tell,\n",
            "Of Baroque architecture, and a culture to compel.\n",
            "\n",
            "The Old Town's a gem, with its cobblestone streets,\n",
            "A UNESCO site, where heritage meets.\n",
            "The Cathedral Square, a sight to behold,\n",
            "With its stunning church, a story to unfold.\n",
            "\n",
            "The Gediminas Tower, a symbol of might,\n",
            "Offers views of the city, a pure delight.\n",
            "The Užupis neighborhood, an artists' haven,\n",
            "A bohemian vibe, where creativity is raven.\n",
            "\n",
            "In Vilnius, the food is a treat to taste,\n",
            "With traditional dishes, and a culinary pace.\n",
            "The city's alive, with a vibrant night scene,\n",
            "A mix of old and new, a true Baltic dream.\n",
            "\n",
            "So if you ever visit, don't be shy,\n",
            "Explore Vilnius, and let its charm get by.\n",
            "You'll fall in love, with its beauty so rare,\n",
            "In Vilnius, Lithuania, a city beyond compare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Amsterdam\"),\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "_qP1h3ahe0DE",
        "outputId": "9f7f3261-c878-4968-c74b-5330d39f7748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amsterdam's a city so fine,\n",
            "In the Netherlands, it truly does shine.\n",
            "Its canals are pretty, its buildings so tall,\n",
            "A great place to visit, for one and for all.\n",
            "\n",
            "The Rijksmuseum's there, with art on display,\n",
            "And the Anne Frank House, to visit each day.\n",
            "The city's got charm, with its streets so so neat,\n",
            "And the people are friendly, with a smile to greet.\n",
            "\n",
            "The Jordaan's a neighborhood, with shops and with flair,\n",
            "And the Vondelpark's a green space, with fresh air to share.\n",
            "The city's got character, with its history so bright,\n",
            "Amsterdam's a destination, that's a pure delight!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt template class we've written is very primitive and would fail if, for example, some keys aren't inputted.\n",
        "\n",
        "One of the good implementations of prompt templates can be found in LangChain [PromptTemplates](https://python.langchain.com/docs/concepts/prompt_templates/)"
      ],
      "metadata": {
        "id": "3nSqLBC2e0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -qU"
      ],
      "metadata": {
        "id": "m6nkay_qe0DE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You only answer in rhymes\"),\n",
        "#    (\"system\", \"Думай по русски, отвечай по русски, отвечай в рифму в стиле Пушкина.\"),\n",
        "    (\"user\", \"Tell me about {city}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"city\": \"Madrid\"})"
      ],
      "metadata": {
        "id": "GeZEj5pCe0DE",
        "outputId": "4857eaee-19e5-4f7e-f062-234756bcb9b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You only answer in rhymes', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about Madrid', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Текст ссылки](https://)**Note:** You don't have to use LangChain llm calls or anything else, you can only take their PromptTemplate implementation.\n",
        "\n",
        "However, there's quiet a bit of useful code in that library."
      ],
      "metadata": {
        "id": "5zweywyae0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import convert_to_openai_messages"
      ],
      "metadata": {
        "id": "8SyzAiTge0DF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templated_messages = convert_to_openai_messages(prompt_template.invoke({\"city\": \"Madrid\"}).to_messages())\n",
        "templated_messages"
      ],
      "metadata": {
        "id": "xvE2qTQce0DF",
        "outputId": "62808e9c-e5d2-4c8a-d786-bafc5fcf804d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You only answer in rhymes'},\n",
              " {'role': 'user', 'content': 'Tell me about Madrid'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=templated_messages,\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "awT49JQve0DF",
        "outputId": "c0c7ce35-9a77-4076-ee64-8e9fa499963c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Madrid, the city's so fair,\n",
            "A capital of Spain, with culture to share.\n",
            "The Prado Museum's a must-see sight,\n",
            "With art from Goya, and Velázquez in sight.\n",
            "\n",
            "The Retiro Park's a green oasis so fine,\n",
            "A place to relax, and let your spirit shine.\n",
            "The Royal Palace's grandeur will take your breath,\n",
            "A symbol of history, and a story to bequeath.\n",
            "\n",
            "Tapas and paella, a culinary delight,\n",
            "Will dance on your taste buds, with flavors so bright.\n",
            "The nightlife's vibrant, with music and cheer,\n",
            "In Madrid, the fun never ends, my dear!\n",
            "\n",
            "The Sierra Nevada's nearby, a mountain so high,\n",
            "A great place for skiing, and a winter sky.\n",
            "The city's warm hospitality, will make you feel at home,\n",
            "In Madrid, the heart of Spain, you'll never feel alone!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structuring LLM outputs\n",
        "\n",
        "In many cases you require not just a free text answer, but something particular you can use later in your system. For example, if you want your LLM to classify a customet's intent to later pass the conversation to a relevant department, you need to extract the particular intent class from the LLM's answer.\n",
        "\n",
        "To parse your LLM outputs conveniently, it's wise to structure them in a specific way. We've already discussed some prompting tricks in Topic 1; this time, we'll learn several more reliable ways of making the LLM abide a deisgnated output format."
      ],
      "metadata": {
        "id": "PLtqUw9ce0DF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic output structuring\n",
        "\n",
        "As a basic way to structure your output, you can \"ask\" an LLM to present the output in a specific format. For example:"
      ],
      "metadata": {
        "id": "Wb8TpmGZe0DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Design one role play character\\'s name, class and a short description.\n",
        "Present it as a markdown list\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "liGqj3PKe0DF",
        "outputId": "0f9bf4c8-747c-4339-8284-f4c6c26e18c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Character Profile**\n",
            "  + Name: Eira Shadowglow\n",
            "  + Class: Moonwhisper Ranger\n",
            "  + Description: A mystical and agile hunter attuned to the whispers of the moon, Eira uses her exceptional archery skills and connection to nature to navigate the shadows and protect the innocent from the forces of darkness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is quite good, it's not very reliable. A better way would be to show some examples to LLM so that it knows what we expect.\n",
        "\n",
        "These examples are known as **few-shot examples** and the prompting technique itself - as **few-shot prompting**."
      ],
      "metadata": {
        "id": "y7U9nI3Ae0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Design one role play character\\'s name, class and a short description. Present it as a markdown list.\\n'\\\n",
        "            \"Examples:\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Randalf the Yellow;\\n\"\\\n",
        "            \"- **Class:** Fire mage;\\n\"\\\n",
        "            \"- **Proficiency:** Pyro magic;\\n\"\\\n",
        "            \"- **Resistance:** Fire;\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Bonan;\\n\"\\\n",
        "            \"- **Class:** Barbarian;\\n\"\\\n",
        "            \"- **Proficiency:** Axe;\\n\"\\\n",
        "            \"- **Resistance:** Mental magic;\\n\"\\\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "I5lonLame0DG",
        "outputId": "f5659c4d-5513-44d7-ac8f-79b0d071dc8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Name:** Eriol the Unyielding;\n",
            "- **Class:** Paladin;\n",
            "- **Proficiency:** Swordsmanship;\n",
            "- **Resistance:** Dark magic;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, LLM captured the format pretty well.\n"
      ],
      "metadata": {
        "id": "PzygVBe0e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Solve the following equation and output only the answer number without reasoning after \"Answer:\"\\n' \\\n",
        "            '9876543210 / 123456789 = ?\\n' \\\n",
        "            'Answer:'\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "S4KXsxtZe0DG",
        "outputId": "216bedaa-dfed-4b2e-f834-cca489af95e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.00000600775 \n",
            "Answer: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ever though the answer isn't correct (LLMs are notoriously bad at arithmetics), the output structure is correct and easy to parse out."
      ],
      "metadata": {
        "id": "BtVmakLFe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can do even better."
      ],
      "metadata": {
        "id": "JNQ0ns6Oe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured outputs\n",
        "\n",
        "Modern LLMs support outputing in a specific format, for example we can use \"JSON mode\" to force outputs to be in JSON fromat."
      ],
      "metadata": {
        "id": "_2G1O-w6e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_output = nebius_client.chat.completions.create(\n",
        "    messages=[{'role': 'user', 'content': 'Design a role play character\\'s name, class and a short description in json format'}],\n",
        "    model=llama_model,\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ").choices[0].message.content\n",
        "json_output"
      ],
      "metadata": {
        "id": "Q88QagE-e0DG",
        "outputId": "0b98c52a-d186-4bd0-dffa-b398a0d7143b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"Eryndor Thorne\", \"class\": \"Shadow Assassin\", \"description\": \"A stealthy and agile warrior with unparalleled skills in darkness and deception, Eryndor navigates the shadows to eliminate his targets with precision and silence.\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful, because that'll make it much easier for you later to parse the outputs:"
      ],
      "metadata": {
        "id": "m4KVrmlFe0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.loads(json_output)"
      ],
      "metadata": {
        "id": "b9-7ouAie0DH",
        "outputId": "b240d96d-0e31-4f4e-c5a1-4b769b09df46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Eryndor Thorne',\n",
              " 'class': 'Shadow Assassin',\n",
              " 'description': 'A stealthy and agile warrior with unparalleled skills in darkness and deception, Eryndor navigates the shadows to eliminate his targets with precision and silence.'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go another step further and actually define a `pydantic` model to create a schema for our outputs:"
      ],
      "metadata": {
        "id": "tTrOKdoee0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "completion = nebius_client.chat.completions.create(\n",
        "    model=llama_model,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Design a role play character\"}\n",
        "    ],\n",
        "    extra_body={\n",
        "        \"guided_json\": CharacterProfile.model_json_schema()\n",
        "    }\n",
        ")\n",
        "\n",
        "CharacterProfile.model_validate_json(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "dki1ECRee0DH",
        "outputId": "cecc596d-fa3e-4b62-d078-fe1b8af90887",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharacterProfile(name='Eira Shadowglow', age=25, special_skills=['Stealth', 'Archery', 'Elemental Magic'], traits=['Agile', 'Resourceful', 'Independent'], character_class='Rogue Mage', origin='The Kingdom of Silverleaf')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So no we have predefined format of outputs, which is easy to work with."
      ],
      "metadata": {
        "id": "2viyoMVOe0DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to structure outputs is using examples\n",
        "\n",
        "Let's consider an example from a famous [MMLU dataset](https://huggingface.co/datasets/cais/mmlu):"
      ],
      "metadata": {
        "id": "TpeDDvNse0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which of the following statements about Ethernets is typically FALSE?\"\n",
        "\n",
        "A = \"Ethernets use circuit switching to send messages.\"\n",
        "B = \"Ethernets use buses with multiple masters.\"\n",
        "C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\"\n",
        "D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\"\n",
        "\n",
        "correct_answer = \"A\""
      ],
      "metadata": {
        "id": "tygKuanMe0DI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally we want our LLM to solve this \"test\" by answering to us with a letter corresponding to the right answer. This will also make calculating metrics much easier. Let's see what would happen."
      ],
      "metadata": {
        "id": "s5vvF33Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(question)\n",
        "print(correct_answer)"
      ],
      "metadata": {
        "id": "xEYZq5BhIFfI",
        "outputId": "f05d7de8-36f4-4c23-f197-f81d0d202048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which of the following statements about Ethernets is typically FALSE?\n",
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "cG2GCODVe0DI",
        "outputId": "ebbb71df-20ef-4dcc-d9b9-2d028e4cd916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Ethernets use circuit switching to send messages.\n",
            "\n",
            "Explanation: Ethernets use packet switching, not circuit switching. Packet switching allows multiple devices to share the same communication channel, whereas circuit switching dedicates a channel to a single connection for the duration of the communication. The other options are true: Ethernets do use buses with multiple masters (option B), they do use collision-detection methods like CSMA/CD (option C), and networks connected by Ethernets are limited in length to a few hundred meters (option D) due to signal degradation. \n",
            "\n",
            "Therefore, the correct answer is: A.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it did output the right answer, but if we do a simple comparison, we'll get into trouble:"
      ],
      "metadata": {
        "id": "TaJNBEK7e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "hIadZJfVe0DI",
        "outputId": "08490f05-71bc-4f21-8761-6281c83ea8b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's teach our model to answer in the right way using so-called Few Shot Prompting also known as In-Context Learning. We essentially show the model some examples in the prompt to teach it in which format we want the answer to be"
      ],
      "metadata": {
        "id": "v6igGq25e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Examples:\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Task:\n",
        "Answer the following question with one of the options listed below. Only ouput the answer in the same format as the examples.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ahOoSsxhe0DI",
        "outputId": "be23bfe0-1dd7-4b47-eaae-233536398be3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "_wlgxkEue0DI",
        "outputId": "5c48cfb3-f978-4240-a2ec-a70abf206283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have observed that for some models the dialog format is actually a better way to structure the Few-Shot examples"
      ],
      "metadata": {
        "id": "uwEVq78Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "Assitant:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WsQW4lNqe0DJ",
        "outputId": "5c83e82a-1b64-42a7-e128-0aecea36d1a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A \n",
            "\n",
            "Ethernets typically use packet switching, not circuit switching. Circuit switching is a method of transmitting data where a dedicated communication path is established between the sender and receiver for the duration of the transmission, whereas Ethernets use a packet-switching method where data is broken into packets and transmitted independently. \n",
            "\n",
            "So, the correct answer is: A: Ethernets use circuit switching to send messages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretically we don't even need to show the model relevant examples if we want it to learn the output formatting"
      ],
      "metadata": {
        "id": "rClIT520e0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Question: Choose the letter A\n",
        "A: A\n",
        "B: B\n",
        "C: C\n",
        "D: D\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which is the biggest number?\n",
        "A: 1\n",
        "B: 2\n",
        "C: 3\n",
        "D: 4\n",
        "Answer:\n",
        "D\n",
        "\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "bUyAniI1e0DJ",
        "outputId": "cb9b4a32-0272-4b17-e1cf-3f3d832899cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "\n",
            "Ethernets typically use packet switching, not circuit switching, to send messages. Circuit switching is a method where a dedicated circuit is established between the sender and receiver for the duration of the communication, which is not how Ethernets operate. Ethernets operate using packet switching, where data is divided into packets and each packet is routed independently through the network. \n",
            "\n",
            "Therefore, the statement that \"Ethernets use circuit switching to send messages\" is typically FALSE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Sometimes you can confuse the model if you have examples from the distribution, which is different than your data's one. So for the best results try to match the distribution."
      ],
      "metadata": {
        "id": "dWhLwogce0DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling\n",
        "\n",
        "We can use tools in OpenAI api as well. Let's see how we can use web search with just the api:"
      ],
      "metadata": {
        "id": "sH-D2q9te0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python -qU"
      ],
      "metadata": {
        "id": "ML_0cLRre0DJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a Tavily API key which you can get from [here](https://app.tavily.com/sign-in).\n",
        "\n",
        "Then either use google's secret storage or put it into a file and upload it."
      ],
      "metadata": {
        "id": "g9r9qF5Qe0DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DS: I'm here and I need API key"
      ],
      "metadata": {
        "id": "rBk52uwWO3aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TAVILITY_API_KEY\"] = open(\".tavily_api_key\").read()\n",
        "#os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"tavily_api_key\")\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "response = tavily_client.search(\"Who is Leo Messi?\", topic=\"general\")\n",
        "\n",
        "print(response['results'])"
      ],
      "metadata": {
        "id": "RjdV-AHye0DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a `tool` description for client, so that the model knows how to use it.\n",
        "\n",
        "We will only expose `query` and `topic` parameters.\n",
        "\n",
        "We also need to write short descriptions to explain what the tool and the parameters are for. Note that it's not for you, but for the LLM :) So please make sure you provide a clear explanation.\n",
        "\n",
        "Tool usage is sort of an extension of \"JSON mode\" because in the end we get a dict of parameters, parsed from the JSON."
      ],
      "metadata": {
        "id": "IWspcVJxe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"web-search\",\n",
        "            \"description\": \"Retrieves results from web search\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"What you search for\",\n",
        "                    },\n",
        "                    \"topic\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Search topic either 'general' or 'news'\",\n",
        "                        \"enum\": [\"general\", \"news\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What is the name of the cat from Shrek?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "Y00ckl3Ie0DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can also try to ask for some news-worthy content to see if LLM decides on a different `topic`."
      ],
      "metadata": {
        "id": "xcU7jtcoe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What happened in London today?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "gx4QpV3Le0DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can extract the function usage output from the result"
      ],
      "metadata": {
        "id": "n94smyzfe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.tool_calls[0]"
      ],
      "metadata": {
        "id": "-AMA-dM9e0DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be wondering, why do we include tool usage in structured output topic.\n",
        "\n",
        "Thing is, you can also use this functionality to structure your output. You don't have to use a real function as your tool. Let's use our previous example"
      ],
      "metadata": {
        "id": "hDqCaytPe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"create_rpg_character\",\n",
        "            \"description\": \"Creates a character based on attributes and description\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Name of the character\",\n",
        "                    },\n",
        "                    \"age\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Age of the character\",\n",
        "                    },\n",
        "                    \"special_skills\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of special skills of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"traits\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of traits of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"character_class\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Class of the character\",\n",
        "                        \"enum\": [\"mage\", \"rogue\", \"barbarian\", \"knight\", \"paladin\"]\n",
        "                    },\n",
        "                    \"origin\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Origin of the character\",\n",
        "                        \"enum\": [\"human\", \"elf\", \"orc\", \"undead\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"name\", \"age\", \"special_skills\", \"traits\", \"character_class\", \"origin\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "ATabJqL4e0DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked to create a character, use `create_rpg_character` tool.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Generate a random character for my new session\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response.choices[0].message.tool_calls[0].function.arguments"
      ],
      "metadata": {
        "id": "uFf2dqmPe0DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice tasks**\n",
        "\n",
        "If you encounter any difficulties or simply want to see our solutions, feel free to check the [Solutions notebook](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/2.1_structured_inputs_and_outputs_solutions.ipynb)."
      ],
      "metadata": {
        "id": "4bPMLxcse0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. LLM Information extraction\n",
        "\n",
        "The goal of this task is to create a system, which extracts data about events from free text into a predictable format."
      ],
      "metadata": {
        "id": "7ZlJaY22e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's imagine that you work for a marketing agency, and you need to gather analytics about the passing events dedicated to AI and Machine Learning. For that, you need to process press releases and extract:\n",
        "- Event name,\n",
        "- Event date,\n",
        "- Number of participants,\n",
        "- Number of speakers,\n",
        "- Attendance price.\n",
        "\n",
        "Of course, you can do it manually, but it's much more fun to use Generative AI! So, your task will be to write a function that does this with only one request to OpenAI API.\n",
        "\n",
        "Below there is an example of a press release (generated by ChatGPT, of course, so that both the event and the personae are fictional). All of them are in the press_releases.zip archive in the hometask week 1 folder.\n",
        "\n",
        "<blockquote>\n",
        "<p>PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence</p>\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact:\n",
        "Jane Cipher\n",
        "Director of Communications, InnovAI Summit\n",
        "Email: jane.cipher@innovai.org\n",
        "Phone: +123-4567-8910</p>\n",
        "</blockquote>\n",
        "\n",
        "More specifically, you should write a function\n",
        "\n",
        "```python\n",
        "parse_press_release(pr: str) -> dict\n",
        "```\n",
        "\n",
        "where the output should be in the format\n",
        "\n",
        "```python\n",
        "{\n",
        "  name: 'InnovAI Summit 2023',\n",
        "  date: '08.11.2023',\n",
        "  n_participants: 3500,\n",
        "  n_speakers: 4,\n",
        "  price:\n",
        "}\n",
        "```\n",
        "\n",
        "If any of the four characteristics is not mentioned in the text, put `None` in the respective field.\n",
        "\n",
        "At the end, calculate the statistics of right answers and analyse what kind of mistakes you \"model\" makes the most."
      ],
      "metadata": {
        "id": "cEvtiZy6e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hints and suggestions:**\n",
        "- It's gonna be more convenient to experiment in Nebius AI Studio's playground https://studio.nebius.com/playground.\n",
        "- You need to be very accurate with what you want from the model.\n",
        "- It will help if you specify in the prompt that the output should be in JSON format, this way you will spend less time parsing the output. But be careful. Though some models are easily prompted to output a JSON, please check the output format. It may contain excessive formatting, for example:\n",
        "<pre><code>```json\n",
        "{\"name\": \"InnovAI Summit 2023\", ...}\n",
        "```</pre></code>\n",
        "Actually, examining LLM outputs and their format is a must when working with them\n",
        "\n",
        "- Please be careful with the details. For example, Jane Cipher in the text above is not a speaker and shouldn't be counter as such (how to get rid of a contact person?). Also pay attention to the date format,\n",
        "- If the model is too wilful with the output format, don't hesitate to show some examples. Decreasing the temperature of predictions can help reduce the creativity of the answer, which is what we want for such task.\n",
        "- Debugging an LLM-powered application may become a tough business. When you think that you've polished it, an LLM can still surprise you. So, we don't expect 100% accuracy in this task, but we expect that you do your best to achieve high quality results."
      ],
      "metadata": {
        "id": "eySv4YHWe0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus points**:\n",
        "Try writing the solution using:\n",
        "- Structured JSON Output\n",
        "- Guiding JSON Output using Structures"
      ],
      "metadata": {
        "id": "vbJYFRc6e0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "press_release = \"\"\"PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\"\"\""
      ],
      "metadata": {
        "id": "zJspUelOe0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_press_release(pr: str) -> dict:\n",
        "    pass"
      ],
      "metadata": {
        "id": "fXP5oO41e0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "We've prepared a small dataset for you to test your prompt on. Provided you've written your function, try running the following code. At the end you also have an opportunity to look at the results in a table side-by-side in with_results.csv. Your goal is to get at least 60% of fields right.."
      ],
      "metadata": {
        "id": "rXJF_b3de0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "!gdown -O press_release_extraction.csv https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv"
      ],
      "metadata": {
        "id": "dTVTe284e0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "pr_df = pandas.read_csv(\"press_release_extraction.csv\")\n",
        "pr_df.head()"
      ],
      "metadata": {
        "id": "OJjcT2pSe0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pr_df.pr_parsed[0]"
      ],
      "metadata": {
        "id": "sDGJ1vKxe0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "parsed_list = []\n",
        "fields = {\n",
        "    \"name\": str,\n",
        "    \"date\": str,\n",
        "    \"n_speakers\": int,\n",
        "    \"n_participants\": int,\n",
        "    \"price\": str\n",
        "}\n",
        "correct_fields = 0\n",
        "for row in pr_df.itertuples():\n",
        "    parsed_release = parse_press_release(row.pr_text)\n",
        "    parsed_list.append(json.dumps(parsed_release, indent=4))\n",
        "    golden = json.loads(row.pr_parsed)\n",
        "    for field, field_type in fields.items():\n",
        "        golden_field = golden[field]\n",
        "        parsed_field = parsed_release.get(field)\n",
        "        try:\n",
        "            parsed_field = field_type(parsed_field)\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "        if golden_field == parsed_field:\n",
        "            correct_fields += 1\n",
        "        else:\n",
        "            print(f\"For {golden['name']} {field} {parsed_release.get(field)} doesn't seem the same as {golden[field]}\")\n",
        "\n",
        "print(f\"Correctly extracted {correct_fields} out of {5*len(pr_df)}\")"
      ],
      "metadata": {
        "id": "RSvVEFCWe0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus points\n",
        "- Try and compare different ways of establishing the correct answer formatting\n",
        "- Try and compare different LLMs"
      ],
      "metadata": {
        "id": "lcGzo04oe0DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Localized MMLU\n",
        "\n",
        "Cool thing about structured output, is that it's very easy to make a translated version of a specific dataset, taking into account all the context and outputing in a format, which is super easy to parse. Let's try this on MMLU.\n",
        "\n",
        "**Task:** Write a function which inputs a sample from MMLU and outputs a translated version, using structured outputs.\n",
        "\n",
        "Tip: make sure that the correct answer didn't change."
      ],
      "metadata": {
        "id": "EDQRdBcLe0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "wsZ7lgLXe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    ...\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    ..."
      ],
      "metadata": {
        "id": "-iMfgJG7e0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    question: str\n",
        "    A: str\n",
        "    B: str\n",
        "    C: str\n",
        "    D: str\n",
        "    correct_answer: str\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    completion = nebius_client.chat.completions.create(\n",
        "        model=llama_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Translate this MMLU sample into {target_language}\" \\\n",
        "                f\"Question: {sample.question}\\n\" \\\n",
        "                f\"A: {sample.A}\\n\" \\\n",
        "                f\"B: {sample.B}\\n\" \\\n",
        "                f\"C: {sample.C}\\n\" \\\n",
        "                f\"D: {sample.D}\\n\" \\\n",
        "                f\"Correct answer: {sample.correct_answer}\\n\" \\\n",
        "                f\"Translated sample:\"\n",
        "            }\n",
        "        ],\n",
        "        extra_body={\n",
        "            \"guided_json\": MMLUSample.model_json_schema()\n",
        "        },\n",
        "    )\n",
        "\n",
        "    translated = MMLUSample.model_validate_json(completion.choices[0].message.content)\n",
        "    if translated.correct_answer != sample.correct_answer:\n",
        "        translated.correct_answer = sample.correct_answer\n",
        "    return translated"
      ],
      "metadata": {
        "id": "eieWNf6Ze0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmlu_sample = MMLUSample(\n",
        "    question = \"Which of the following statements about Ethernets is typically FALSE?\",\n",
        "    A = \"Ethernets use circuit switching to send messages.\",\n",
        "    B = \"Ethernets use buses with multiple masters.\",\n",
        "    C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\",\n",
        "    D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\",\n",
        "    correct_answer = \"A\"\n",
        ")\n",
        "\n",
        "translate_mmlu_sample(mmlu_sample, target_language=\"German\")"
      ],
      "metadata": {
        "id": "6EqyzfJxe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's remember the code we've written for MMLU evaluator and add a little twist:\n",
        "\n",
        "We'll have both topic and language in which we want to evaluate the model."
      ],
      "metadata": {
        "id": "uwoVgXi7e0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "KIB_2RFYe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Modify the following MMLUEvaluator code so that it can also translate the input question and evaluate the performance in a different language."
      ],
      "metadata": {
        "id": "TIqzv_AJe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "\n",
        "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.split('#ANSWER:')[1].strip()\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    client=client, model=model,\n",
        "                    topic_prettified=self.topic_prettified,\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "                ),\n",
        "                system_prompt=self.system_prompt,\n",
        "                prettify=False\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client=nebius_client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                       n_questions=50) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results\n"
      ],
      "metadata": {
        "id": "KmbtCJnNe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "78Eqh6jhe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MMLUEvaluator(topic=\"medical_genetics\", language=\"English\")\n",
        "\n",
        "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=50)\n",
        "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
      ],
      "metadata": {
        "id": "DpQ9Lp6_e0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_de = MMLUEvaluator(topic=\"medical_genetics\", language=\"German\")\n",
        "\n",
        "results_de = evaluator_de.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=10)\n",
        "print(f'\\nAccuracy: {results_de[\"accuracy\"]}')"
      ],
      "metadata": {
        "id": "-_jrw-z0e0DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIW2nMg8e0DN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}